{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time \n",
    "import pwd\n",
    "import re\n",
    "import nltk\n",
    "ximport plotly.express as px\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words_time = ['date', 'time', 'year']\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"data_urban.csv\"\n",
    "file_path = \"../Data/\" + file_name\n",
    "if file_name.endswith('.csv'):\n",
    "    data = pd.read_csv(file_path)\n",
    "st = os.stat(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Sensor Name        Variable   Units            Timestamp  \\\n",
      "0  PER_AIRMON_MESH1907150  Particle Count  Kgm -3  2019-10-13 22:07:00   \n",
      "1  PER_AIRMON_MESH1965150              O3     ppb  2019-10-13 22:07:00   \n",
      "2  PER_AIRMON_MESH1965150             NO2  ugm -3  2019-10-13 22:07:00   \n",
      "3  PER_AIRMON_MESH1965150              NO  ugm -3  2019-10-13 22:07:00   \n",
      "4  PER_AIRMON_MESH1965150              CO  ugm -3  2019-10-13 22:07:00   \n",
      "\n",
      "       Value  Flagged as Suspect Reading  \n",
      "0    0.39000                       False  \n",
      "1   16.84500                       False  \n",
      "2   20.19872                       False  \n",
      "3   11.43500                       False  \n",
      "4  461.33424                       False  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ana are  mere'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "def remove_punctuation(text):\n",
    "    text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "    return text\n",
    "\n",
    "my_string = \"ana are, mere\"\n",
    "remove_punctuation(my_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor Name\n",
      "Variable\n",
      "Units\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-af355ec5d459>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mdf_keywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'keywords'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdf_keywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'keywords'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'str'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_keywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "#Filter not date/is string\n",
    "def filter_columns(req_string): \n",
    "    pattern  = re.compile(r'' + \"|\".join(stop_words_time), re.IGNORECASE)\n",
    "    is_string_column = lambda x : data[x].dtypes == 'O'\n",
    "    return not bool(pattern.search(req_string)) and is_string_column(req_string)\n",
    "\n",
    "df_nlp = pd.DataFrame()\n",
    "keywords = list()\n",
    "for column in filter(filter_columns, data.columns):\n",
    "    #Tokenize and lower case\n",
    "    df_nlp[column] = data[column].apply(lambda x : x.lower().split(\" \"))\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase = False, ngram_range = (1,2))\n",
    "    tfidf_matrix = vectorizer.fit_transform(df_nlp[column].astype(str).tolist())\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    print(column)\n",
    "    #Get the words that appear\n",
    "    feature_indexes = tfidf_matrix[:,:].nonzero()[1]\n",
    "    df = pd.DataFrame(tfidf_matrix.toarray(), columns = vectorizer.get_feature_names())\n",
    "    df = df.apply(lambda x : x.mean()).sort_values(ascending=False)\n",
    "    stat, p = shapiro(df)\n",
    "    alpha = 0.05\n",
    "    if p <= alpha:\n",
    "        keywords.append((df[df > df.describe()['max'] - df.describe()['std']].index.tolist()))\n",
    "        \n",
    "df_keywords = pd.DataFrame(columns = ['keywords'])\n",
    "df_keywords.loc[0, 'keywords'] = \"\\n\".join(map(str, keywords)).astype('str') \n",
    "print(df_keywords.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
